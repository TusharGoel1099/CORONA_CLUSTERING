# -*- coding: utf-8 -*-
"""corona_india_clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vv8x1zTGgsviGzI9X2Djb3X76tfw6thI
"""



start_data=pd.read_csv("covid.csv")
start_data.head()
print("RAW DATA FROM WHO WEBSITE")



# ********************************* CONVERTING EACH AND EVERY DISTRICT TO ITS GEO COORDINATES USING GEOPANDAS AND GEOPY *********
# 1) THIS FUNCTION IS USED TO PREPROCESS ALL THE 4400 DISTRICT NAMES INTO TWO COLUNMS LATITUDE AND LONGITUDE
#count=0
#xx=[]
#import time
#yy=[]
#for i in df["district"][2000:2600]:
#  try:
#    geolocator = Nominatim(user_agent="geoapiExercises")
#    location = geolocator.geocode(i)
#    xx.append(location.latitude)
#    yy.append(location.longitude)
#    print(location.latitude,location.longitude)
#    print(i)
#  except:
#    xx.append(19.0760)
#    yy.append(72.8777)
#    print("********************************************************")
#    print(i)
#  count+=1
#  time.sleep(1)

#*******************  ALL OF THIS IS USED TO PREPROCESS THE DATA ****************************************
1) #USING FILLNA TO FILL ALL THE NULL VALUES OF AGE WITH THE MEAN
2) #FILLING THE NAN GENDER COLUNM
3)  #LABEL ENCODING THE GENDER COLUNM TO NUMERICAL BINARY 0,1 
4)  #REMOVING UNWANTED COLUNMS
5)  #ADDING POPULATION COLUNM WITH RESPECTIVE TO EACH AND EVERY DISTRICT USING ANOTHER DATASET OF CENSUS 2017 INDIA
6)  #SAVING BACK ALL PREPROCESSED DATA TO COVID_DATA.CSV

# df=pd.read_csv("covid_data.csv")
# df1=pd.read_csv("C:/Users/Tushar goel/Desktop/covid.csv")
# df['age']=df1['age']
# df["age"]=df["age"].fillna(df["age"].mean())
# df["gender"]=df["gender"].fillna('M')
# from sklearn.preprocessing import LabelEncoder
# en=LabelEncoder()
# df["gender"]=en.fit_transform(df["gender"])
# df.rename(columns={"city": "longitude", "district": "latitude"},inplace=True)
# df.drop(['Unnamed: 0'],inplace=True,axis=1)
# df["longitude"].replace({11.0480288: 77.4126}, inplace=True)
# df["latitude"].replace({46.3144754: 23.2599}, inplace=True)
# df.to_csv("covid_data.csv")
# df2=pd.read_csv("district.csv")
# dic=dict()
# list2=[]
# for j in df1['district']:
#          flag=1
#          for i in zip(df2['District name'],df2['Population']):
#                  if(i[0]==j):
#                      dic[j]=i[1]
#                      list2.append(i[1])
#                      flag=0
#          if(flag==1):
#              dic[j]=2255525
#              list2.append(2255525)
# df5=pd.read_csv("covid_data.csv")
# df5['population']=list2[0:4399]
# df5.drop("Unnamed: 0",inplace=True,axis=1)
# df5.to_csv("covid_data.csv")



from geopy.geocoders import Nominatim
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
df=pd.read_csv("covid_data.csv")
print("AFTER PROCESSING MY DATA LOOKS LIKE")
df=df.drop(['Unnamed: 0'],axis=1)
print(df.head())
plt.scatter(df['latitude'],df['longitude'],color='blue',s=100)
plt.xlabel("latitude")
plt.ylabel("longitude")
plt.title("scatter geo-coordinates")
plt.xlim(0,100)
plt.ylim(20,100)
plt.show()
check=pd.DataFrame(df['state'])
df=df.drop(['state'],axis=1)
from sklearn.preprocessing import StandardScaler
mx=StandardScaler()
df= pd.DataFrame(mx.fit_transform(df), columns=df.columns)#scaling data
print("DATA AFTER STANDARD SCALING")
print(df.head())
from sklearn.decomposition import PCA
pca = PCA(n_components=2) #APPLYING PRINCIPAL COMONENT ANALYSIS TO TAKE FIRST 2 VECTORS WITH HIGHEST MAGNITUDE
principalComponents = pca.fit_transform(df)
principalDf = pd.DataFrame(data = principalComponents
             , columns = ['component_1', 'component_2'])
print("MY NUMBER OF FEATURES REDUCED TO 2 DIMENTIONAL")
print(principalDf.head())

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
#CHECKING THE PERFECT NUMBER OF CLUSTERS
df3=pd.DataFrame({'a':principalDf["component_1"],'b':principalDf["component_2"]})
dist=[]
dist2=[]
for i in range(2,20):
  model=KMeans(n_clusters=i)
  model.fit(principalDf)
  df3[f"label{i}"]=model.predict(principalDf)
  di=silhouette_score(df3[["a","b",f"label{i}"]],df3[f"label{i}"])
  dist2.append(di)
  dist.append(model.inertia_)
plt.plot(range(2,20),dist,color="red",label="change in clustering")#USING ELBOW TO CURVE TO DETECT THE CONSTANT CHANGE
plt.xlabel("cluster")
plt.title("elbow curve")
plt.ylabel("inertia")
plt.legend()
plt.show()
plt.bar(range(2,20),dist2,color="MAGENTA",label="change in silhouette score")#IT DETECT THE AVERAGE DISTANCE OF DATAPOINTS FROM THEIR CLUSTER TO THE NEIGBOURING CLUSTER
plt.legend()
plt.xlabel("number of clusters")
plt.ylabel("silhouette score")
plt.show()

kmeans = KMeans(n_clusters=11,n_init=11,random_state=10)#APPLYING KMEANS WITH 10 CLUSTERS
model=kmeans.fit(principalDf)
print(kmeans.inertia_)
principalDf['labels']=model.predict(principalDf)
df['labels']=principalDf['labels']
check['labels']=principalDf['labels']
x=kmeans.cluster_centers_#THE COMPONENTS OF CENTRIODS OF CLUSTERS
plt.scatter(principalDf["component_1"][principalDf['labels']==0],principalDf["component_2"][principalDf['labels']==0],color='red',label="cluster_0")
plt.scatter(principalDf["component_1"][principalDf['labels']==1],principalDf["component_2"][principalDf['labels']==1],color='blue',label="cluster_1")
plt.scatter(principalDf["component_1"][principalDf['labels']==2],principalDf["component_2"][principalDf['labels']==2],color="black",label="cluster_2")
plt.scatter(principalDf["component_1"][principalDf['labels']==3],principalDf["component_2"][principalDf['labels']==3],color="cyan",label="cluster_3")
plt.scatter(principalDf["component_1"][principalDf['labels']==4],principalDf["component_2"][principalDf['labels']==4],color="yellow",label="cluster_4")
plt.scatter(principalDf["component_1"][principalDf['labels']==5],principalDf["component_2"][principalDf['labels']==5],color="magenta",label="cluster_5")
plt.scatter(principalDf["component_1"][principalDf['labels']==6],principalDf["component_2"][principalDf['labels']==6],color="lawngreen",label="cluster_6")
plt.scatter(principalDf["component_1"][principalDf['labels']==7],principalDf["component_2"][principalDf['labels']==7],color="brown",label="cluster_7")
plt.scatter(principalDf["component_1"][principalDf['labels']==8],principalDf["component_2"][principalDf['labels']==8],color="indigo",label="cluster_8")
plt.scatter(principalDf["component_1"][principalDf['labels']==9],principalDf["component_2"][principalDf['labels']==9],color="gray",label="cluster_9")
plt.scatter(principalDf["component_1"][principalDf['labels']==10],principalDf["component_2"][principalDf['labels']==10],color="orange",label="cluster_10")
plt.scatter(x[:,0:1],x[:,1:2],marker="*",color="green",s=200,label="centriods")
plt.title("visualizing clusters")
plt.xlabel("PCA component_1")
plt.ylabel("PCA component_1")
plt.legend()
plt.show()
print("")
from sklearn.metrics import silhouette_score
print(principalDf.head())
x=silhouette_score(principalDf, principalDf['labels'])#CALCULATING SILHOUETTE SCORE FOR 7 CLUSTERS
print(f"silhouette_score: {x}")
print("The number of patients lie in cluster with repect to their assigned label")
principalDf['labels'].value_counts()

plt.bar(check['state'][:4399],check['labels'][:4399],label="all patients",color="yellow")
plt.legend()
plt.xticks(rotation=90)
plt.show()

print(df.head())
x=df[['latitude','longitude']][df['labels']==2]
plt.plot(x)
plt.show()
import seaborn as sns
heatmap1_data = pd.pivot_table(check, values='labels', 
                     index=['state'])
sns.heatmap(heatmap1_data)